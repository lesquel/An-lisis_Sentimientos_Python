from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification


class MiningEngine:
    """
    Motor de MinerÃ­a de Texto basado en Transformers.
    PatrÃ³n Singleton para cargar el modelo en memoria una sola vez.
    
    CaracterÃ­sticas:
    - ClasificaciÃ³n Zero-Shot multilingÃ¼e (optimizado para espaÃ±ol)
    - Soporte multi-label (un post puede tener mÃºltiples emociones)
    - Umbral configurable para detectar emociones secundarias
    """
    
    # Lista expandida de categorÃ­as para la red social (25 categorÃ­as)
    TAXONOMY = [
        # Emociones bÃ¡sicas
        "AlegrÃ­a", "Tristeza", "Enojo", "Miedo", "Sorpresa", "Asco",
        # Emociones sociales
        "Amor", "Odio", "VergÃ¼enza", "Orgullo", "Envidia", "Celos",
        # Tipos de contenido
        "Humor", "InspiraciÃ³n", "ConfesiÃ³n", "Queja", "Consejo",
        "Pregunta", "ReflexiÃ³n", "Nostalgia", "Ansiedad", "FrustraciÃ³n",
        # Contenido especial
        "Sarcasmo", "PolÃ©mica", "Terror"
    ]
    
    # Mapeo de etiquetas a hypothesis templates en espaÃ±ol para mejor precisiÃ³n
    HYPOTHESIS_TEMPLATE = "Este texto expresa {}"
    
    # Umbral mÃ­nimo de confianza (porcentaje del score mÃ¡ximo)
    # Una emociÃ³n secundaria debe tener al menos 97% del score de la primaria
    RELATIVE_THRESHOLD = 0.97
    
    # MÃ¡ximo de emociones a retornar
    MAX_EMOTIONS = 3

    _classifier = None

    @classmethod
    def get_classifier(cls):
        if cls._classifier is None:
            print("ðŸ§  Cargando modelo neuronal multilingÃ¼e... (esto pasa solo una vez)")
            
            # Lista de modelos a intentar (en orden de preferencia)
            models_to_try = [
                ("joeddav/xlm-roberta-large-xnli", True),   # (nombre, usa sentencepiece)
                ("facebook/bart-large-mnli", False),
                ("typeform/distilbert-base-uncased-mnli", False),  # MÃ¡s ligero
            ]
            
            last_error = None
            loaded_model = None
            
            for model_name, use_slow_tokenizer in models_to_try:
                try:
                    print(f"ðŸ“¦ Intentando cargar: {model_name}")
                    
                    if use_slow_tokenizer:
                        tokenizer = AutoTokenizer.from_pretrained(
                            model_name, 
                            use_fast=False,
                            local_files_only=False
                        )
                        cls._classifier = pipeline(
                            "zero-shot-classification",
                            model=model_name,
                            tokenizer=tokenizer,
                            device=-1
                        )
                    else:
                        cls._classifier = pipeline(
                            "zero-shot-classification",
                            model=model_name,
                            device=-1
                        )
                    
                    loaded_model = model_name
                    print(f"âœ… Modelo {model_name} cargado exitosamente!")
                    break
                    
                except Exception as e:
                    last_error = e
                    print(f"âš ï¸ Error cargando {model_name}: {e}")
                    continue
            
            if cls._classifier is None:
                raise RuntimeError(
                    f"No se pudo cargar ningÃºn modelo de clasificaciÃ³n. Ãšltimo error: {last_error}"
                )
        
        return cls._classifier

    @classmethod
    def analyze(cls, text: str) -> dict:
        """
        Analiza un texto y retorna mÃºltiples emociones detectadas.
        
        Returns:
            dict: {
                "categories": ["AlegrÃ­a", "Humor"],  # Lista de categorÃ­as detectadas
                "primary_category": "AlegrÃ­a",       # CategorÃ­a principal
                "primary_confidence": 0.85,          # Confianza de la principal
                "all_scores": {...}                  # Todos los scores
            }
        """
        classifier = cls.get_classifier()
        
        # Inferencia con multi_label=True para detectar mÃºltiples emociones
        result = classifier(
            text, 
            cls.TAXONOMY, 
            hypothesis_template=cls.HYPOTHESIS_TEMPLATE,
            multi_label=True  # Â¡Permite mÃºltiples categorÃ­as!
        )
        
        # Crear diccionario de scores
        all_scores = dict(zip(result['labels'], result['scores']))
        
        # Obtener el score mÃ¡ximo para calcular umbrales relativos
        max_score = result['scores'][0]
        threshold = max_score * cls.RELATIVE_THRESHOLD
        
        # Filtrar emociones que superen el umbral relativo
        detected_categories = []
        for label, score in zip(result['labels'], result['scores']):
            if score >= threshold and len(detected_categories) < cls.MAX_EMOTIONS:
                detected_categories.append({
                    "name": label,
                    "confidence": score
                })
        
        # Si ninguna supera el umbral, tomar la mÃ¡s alta
        if not detected_categories:
            detected_categories = [{
                "name": result['labels'][0],
                "confidence": result['scores'][0]
            }]
        
        return {
            "categories": detected_categories,  # Lista de categorÃ­as con confianza
            "primary_category": result['labels'][0],
            "primary_confidence": result['scores'][0],
            "all_scores": all_scores,
            # Mantener compatibilidad con versiÃ³n anterior
            "top_category": result['labels'][0],
            "confidence": result['scores'][0]
        }
